{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q transformers datasets peft accelerate bitsandbytes pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD before: /workspace\n",
      "CWD now   : /workspace\n",
      "CACHE_DIR : /workspace/hf_cache\n",
      "MODEL_DIR : /workspace/qwen2.5_coder_14b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = \"/workspace\"\n",
    "CACHE_DIR = f\"{BASE_DIR}/hf_cache\"\n",
    "MODEL_DIR = f\"{BASE_DIR}/qwen2.5_coder_14b\"   # optional local save for later reuse\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"CWD before:\", os.getcwd())\n",
    "os.chdir(BASE_DIR)\n",
    "print(\"CWD now   :\", os.getcwd())\n",
    "print(\"CACHE_DIR :\", CACHE_DIR)\n",
    "print(\"MODEL_DIR :\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_HOME              = /workspace/hf_cache\n",
      "TRANSFORMERS_CACHE   = /workspace/hf_cache\n",
      "HF_DATASETS_CACHE    = /workspace/hf_cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = CACHE_DIR\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = CACHE_DIR\n",
    "\n",
    "print(\"HF_HOME              =\", os.environ[\"HF_HOME\"])\n",
    "print(\"TRANSFORMERS_CACHE   =\", os.environ[\"TRANSFORMERS_CACHE\"])\n",
    "print(\"HF_DATASETS_CACHE    =\", os.environ[\"HF_DATASETS_CACHE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found: /root/.cache/huggingface\n",
      "Not found: /root/.cache/huggingface\n",
      "Not found: /root/.local/share/huggingface\n",
      "Removed: /root/.cache/pip\n",
      "Not found: /root/.local/share/pip\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "paths = [\n",
    "    os.path.expanduser(\"~/.cache/huggingface\"),\n",
    "    \"/root/.cache/huggingface\",\n",
    "    \"/root/.local/share/huggingface\",\n",
    "    os.path.expanduser(\"~/.cache/pip\"),\n",
    "    os.path.expanduser(\"~/.local/share/pip\"),\n",
    "]\n",
    "\n",
    "for p in paths:\n",
    "    try:\n",
    "        if os.path.exists(p):\n",
    "            shutil.rmtree(p, ignore_errors=True)\n",
    "            print(\"Removed:\", p)\n",
    "        else:\n",
    "            print(\"Not found:\", p)\n",
    "    except Exception as e:\n",
    "        print(\"Skip (no permission?):\", p, \"->\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q --upgrade transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afea611dac1e4397917ab80b575ba0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49db6b6060c5404a9d6601c63be40309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec0e38b820d478b84d3200e1f3398bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330308b546ce4fecb020ff8645939e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c342b963caa54e28b83e376f80c43aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd0a2e3e40c463990a8d245b0ca1a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37da2fcce1714ad89db21d07154bddc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model & tokenizer ready.\n",
      "GPU: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "# ‚úÖ Cache directory (so models don‚Äôt fill up the 30GB root overlay)\n",
    "CACHE_DIR = \"/workspace/hf_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Model name\n",
    "model_name = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "\n",
    "# ‚úÖ BitsAndBytes 4-bit quantization config (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16  # use fp16 (bf16 not needed)\n",
    ")\n",
    "\n",
    "# ‚úÖ Tokenizer (Qwen2.5 still may need trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set pad_token safely\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ‚úÖ Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",        # automatically places layers across GPUs if multi-GPU\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model & tokenizer ready.\")\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guideline = (\n",
    "    \"You are an ULTIMATE Python Coding Expert. Follow the rules STRICTLY.\\n\"\n",
    "    \"Input: A programming instruction in English with function name and parameters.\\n\"\n",
    "    \"Output: Only the correct Python code inside a fenced code block.\\n\\n\"\n",
    "    \"Format:\\n\"\n",
    "    \"```python\\n\"\n",
    "    \"<code here>\\n\"\n",
    "    \"```\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- Reason internally; output only the final code block.\\n\"\n",
    "    \"- Handle edge cases; the code MUST pass typical unit tests.\\n\"\n",
    "    \"- Preserve the exact function name and parameters.\\n\"\n",
    "    \"- Always return the output (no print() / input()).\\n\"\n",
    "    \"- Do not define classes; use functions and variables only.\\n\"\n",
    "    \"- Import required libraries if needed (no unused imports).\\n\"\n",
    "    \"- Do not include any text, explanations, comments, or docstrings.\\n\"\n",
    "    \"\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Templating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:00<00:00, 3722.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chat template applied ‚Üí /workspace/train_chat_templated.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 5 ‚Äî Apply Qwen chat template (system, user, assistant) ‚Äî RunPod version\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Paths for RunPod\n",
    "BASE_DIR   = \"/workspace\"\n",
    "CACHE_DIR  = os.getenv(\"HF_HOME\", f\"{BASE_DIR}/hf_cache\")\n",
    "QWEN_MODEL_ID = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "\n",
    "INPUT_PATH  = f\"trial_mbpp_cleaned_data.csv\"         # your cleaned CSV\n",
    "OUTPUT_PATH = f\"{BASE_DIR}/train_chat_templated.jsonl\"       # where to write JSONL\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# Load tokenizer (Qwen2.5 typically needs trust_remote_code)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# Load dataset (expects columns: instruction | response)\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "\n",
    "###\n",
    "# df = df.sample(50)\n",
    "\n",
    "\n",
    "def render_row(inst: str, resp: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\",   \"content\": guideline},\n",
    "        {\"role\": \"user\",     \"content\": (inst or \"\").strip()},\n",
    "        {\"role\": \"assistant\",\"content\": (resp or \"\").strip()},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False  # include assistant content in rendered text\n",
    "    )\n",
    "\n",
    "# Stream to jsonl\n",
    "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in tqdm(df.itertuples(index=False), total=len(df), desc=\"Templating\"):\n",
    "        text = render_row(getattr(row, \"instruction\"), getattr(row, \"response\"))\n",
    "        f.write(json.dumps({\"text\": text}, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Chat template applied ‚Üí\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c8858de4748ff95ecb0b0abe266e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db03ad0ed7c34e0684c05b1e043422f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005e45901cfd42e5bc4b101709020c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved tokenized dataset ‚Üí /workspace/ds_tokenized\n"
     ]
    }
   ],
   "source": [
    "# STEP 6 ‚Äî Tokenize (and optionally pack) ‚Äî RunPod version\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Paths for RunPod\n",
    "BASE_DIR   = \"/workspace\"\n",
    "CACHE_DIR  = os.getenv(\"HF_HOME\", f\"{BASE_DIR}/hf_cache\")\n",
    "QWEN_MODEL_ID = \"Qwen/Qwen2.5-Coder-14B-Instruct\"   # keep consistent with Step 5\n",
    "INPUT_PATH = f\"{BASE_DIR}/train_chat_templated.jsonl\"\n",
    "OUTPUT_PATH = f\"{BASE_DIR}/ds_tokenized\"\n",
    "MAX_SEQ_LEN = 1024\n",
    "PACK = False  # set True later if you want packed LM training\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load chat-formatted jsonl -> HF dataset\n",
    "ds = load_dataset(\"json\", data_files=INPUT_PATH, split=\"train\")\n",
    "\n",
    "# Tokenization function\n",
    "def tok_fn(batch):\n",
    "    out = tokenizer(\n",
    "        batch[\"text\"],\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        truncation=True,\n",
    "        padding=False,               # dynamic padding via collator later\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    # For causal LM, labels = input_ids\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "# Apply tokenizer\n",
    "ds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "print(ds_tok)\n",
    "\n",
    "# (Optional) Packing ‚Äî disabled by default\n",
    "if PACK:\n",
    "    block_size = MAX_SEQ_LEN\n",
    "    def group_texts(examples):\n",
    "        concatenated = {k: sum(examples[k], []) for k in [\"input_ids\", \"attention_mask\", \"labels\"]}\n",
    "        total_length = len(concatenated[\"input_ids\"])\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated.items()\n",
    "        }\n",
    "        return result\n",
    "    ds_tok = ds_tok.map(group_texts, batched=True)\n",
    "    print(\"‚úÖ Packed dataset:\", ds_tok)\n",
    "\n",
    "# Save tokenized dataset for reuse\n",
    "ds_tok.save_to_disk(OUTPUT_PATH)\n",
    "print(\"‚úÖ Saved tokenized dataset ‚Üí\", OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training samples: 50\n"
     ]
    }
   ],
   "source": [
    "# STEP 7 ‚Äî Use the whole dataset for training (no val split)\n",
    "# (Nothing to do beyond naming it explicitly)\n",
    "\n",
    "train_dataset = ds_tok\n",
    "print(\"‚úÖ Training samples:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7bbfa116374838b8ad2e744eef59c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257c5877623549b09c4fdc2b1bb1ef5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b3f8db0fd24302a5ab633810a5e6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded in 4-bit with compute dtype: torch.bfloat16\n",
      "GPU: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "# STEP 8 ‚Äî Load model in 4-bit (QLoRA-ready) ‚Äî RunPod version\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "# Paths and model\n",
    "BASE_DIR   = \"/workspace\"\n",
    "CACHE_DIR  = os.getenv(\"HF_HOME\", f\"{BASE_DIR}/hf_cache\")\n",
    "QWEN_MODEL_ID = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
    "\n",
    "# Compute dtype: use bf16 if supported, else fp16 (T4 will fall back to fp16)\n",
    "compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "# 4-bit quantization config for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model in 4-bit mode\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "# Ensure pad id is set on model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(\"‚úÖ Model loaded in 4-bit with compute dtype:\", compute_dtype)\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gradient checkpointing ON; use_cache=False; inputs require grads.\n"
     ]
    }
   ],
   "source": [
    "# STEP 9 ‚Äî Enable gradient checkpointing + training flags for QLoRA\n",
    "\n",
    "# ‚úÖ Reduce memory by recomputing activations\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# ‚úÖ Disable caching (must be False when training with gradient checkpointing)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# ‚úÖ Ensure inputs require gradients (needed for QLoRA / PEFT on 4-bit models)\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"‚úÖ Gradient checkpointing ON; use_cache=False; inputs require grads.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA config ready with r = 8 alpha = 16\n"
     ]
    }
   ],
   "source": [
    "# STEP 10 ‚Äî Define LoRA configuration for QLoRA fine-tuning\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# üîß LoRA hyperparameters\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# üîë Target modules (common for Qwen/transformer-based LLMs)\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "# ‚úÖ Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=TARGET_MODULES,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA config ready with r =\", LORA_R, \"alpha =\", LORA_ALPHA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 4,399,104 | Total params: 319,518,592 | Trainable%: 1.3768%\n",
      "‚úÖ PEFT adapters attached and model is ready for training.\n"
     ]
    }
   ],
   "source": [
    "# STEP 11 ‚Äî Attach LoRA adapters with PEFT\n",
    "from peft import get_peft_model\n",
    "\n",
    "# Wrap the model with LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# (Optional) sanity check: trainable vs total parameters\n",
    "def print_trainable_parameters(m):\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for _, p in m.named_parameters():\n",
    "        num = p.numel()\n",
    "        total += num\n",
    "        if p.requires_grad:\n",
    "            trainable += num\n",
    "    print(f\"Trainable params: {trainable:,} | Total params: {total:,} | \"\n",
    "          f\"Trainable%: {100 * trainable/total:.4f}%\")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "print(\"‚úÖ PEFT adapters attached and model is ready for training.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TrainingArguments ready ‚Üí /workspace/qwen2.5-14b-qlora\n"
     ]
    }
   ],
   "source": [
    "# STEP 12 ‚Äî TrainingArguments for QLoRA fine-tuning on RunPod\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "# ‚úÖ Output directory in /workspace (not /kaggle)\n",
    "OUTPUT_DIR = \"/workspace/qwen2.5-14b-qlora\"\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,   # keep small, accumulate instead\n",
    "    gradient_accumulation_steps=4,  # effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.0,\n",
    "    fp16=True,                       # use mixed precision\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "\n",
    "    # ‚úÖ Logging & checkpointing\n",
    "    logging_strategy=\"epoch\",        # print one line per epoch\n",
    "    save_strategy=\"epoch\",           # save at each epoch\n",
    "    save_total_limit=2,              # keep only last 2 checkpoints (saves disk)\n",
    "    logging_steps=50,                # only used if logging_strategy=\"steps\"\n",
    "\n",
    "    # ‚úÖ Performance\n",
    "    dataloader_num_workers=4,\n",
    "\n",
    "    # ‚úÖ Disable external loggers (use WandB if needed)\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ TrainingArguments ready ‚Üí\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Trainer initialized (dynamic padding + labels).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_165/2241703918.py:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# STEP 13 ‚Äî Initialize Trainer (dynamic padding; labels created by collator)\n",
    "from datasets import load_from_disk\n",
    "from transformers import DataCollatorForLanguageModeling, Trainer\n",
    "\n",
    "# ‚úÖ Reload tokenized dataset saved in Step 6 (RunPod path)\n",
    "train_dataset = load_from_disk(\"/workspace/ds_tokenized\")\n",
    "\n",
    "# ‚úÖ Drop precomputed labels to avoid mismatch when using dynamic padding\n",
    "if \"labels\" in train_dataset.column_names:\n",
    "    train_dataset = train_dataset.remove_columns(\"labels\")\n",
    "\n",
    "# ‚úÖ Ensure right-padding for causal LM\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# ‚úÖ Collator: pads dynamically and sets labels = input_ids (mlm=False)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8,   # speeds up training on tensor cores (A100)\n",
    ")\n",
    "\n",
    "# ‚úÖ Trainer: links model, args, dataset, tokenizer, collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_dataset,   # full dataset (no validation split)\n",
    "    eval_dataset=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized (dynamic padding + labels).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.038800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training finished.\n",
      "TrainOutput(global_step=4, training_loss=3.0387957096099854, metrics={'train_runtime': 21.7074, 'train_samples_per_second': 2.303, 'train_steps_per_second': 0.184, 'total_flos': 25859326089216.0, 'train_loss': 3.0387957096099854, 'epoch': 1.0})\n"
     ]
    }
   ],
   "source": [
    "# STEP 14 ‚Äî Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Save PEFT adapters and tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"‚úÖ Training finished.\")\n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned model (base + LoRA) ready for inference.\n",
      "def sum_series(n):\n",
      "    # Initialize the sum variable\n",
      "    total_sum = 0\n",
      "    \n",
      "    # Loop through the range from 1 to n\n",
      "    for i in range(1, n + 1):\n",
      "        # Add the current number to the total sum\n",
      "        total_sum += i\n",
      "    \n",
      "    return total_sum\n"
     ]
    }
   ],
   "source": [
    "# STEP ‚Äî Reload fine-tuned model (base + LoRA adapters) and run inference\n",
    "import os, re, json, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# ‚úÖ Model & paths\n",
    "BASE_DIR     = \"/workspace\"\n",
    "CACHE_DIR    = os.getenv(\"HF_HOME\", f\"{BASE_DIR}/hf_cache\")\n",
    "QWEN_MODEL_ID = \"Qwen/Qwen2.5-Coder-14B-Instruct\"   # use same as training\n",
    "OUTPUT_DIR    = f\"{BASE_DIR}/qwen2.5-14b-qlora\"    # LoRA adapter save dir from training\n",
    "\n",
    "# ‚úÖ Compute dtype\n",
    "compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "\n",
    "# ‚úÖ Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype\n",
    ")\n",
    "\n",
    "# ‚úÖ Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# ‚úÖ Reload base model in 4-bit\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# ‚úÖ Attach trained LoRA adapters\n",
    "model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "model.eval()\n",
    "print(\"‚úÖ Fine-tuned model (base + LoRA) ready for inference.\")\n",
    "\n",
    "# ===== Prompt rendering helpers =====\n",
    "def render_prompt(instruction: str) -> str:\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": guideline},  # replace with your guideline\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        msgs,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# Extract code block from model output\n",
    "CODE_BLOCK_RE = re.compile(r\"```python\\s*([\\s\\S]*?)```\", re.IGNORECASE)\n",
    "def extract_code(text: str) -> str:\n",
    "    m = CODE_BLOCK_RE.search(text)\n",
    "    return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "# ===== Inference function =====\n",
    "@torch.inference_mode()\n",
    "def generate_code(instruction: str,\n",
    "                  max_new_tokens=512,\n",
    "                  temperature=0.2,\n",
    "                  top_p=0.9,\n",
    "                  rep_penalty=1.05):\n",
    "    text = render_prompt(instruction)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=rep_penalty,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    gen = tokenizer.decode(out_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return extract_code(gen)\n",
    "\n",
    "# ===== Quick sanity check =====\n",
    "print(generate_code(\"Write a function sum_series(n) that returns the sum of first n natural numbers.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "PEFT config: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='Qwen/Qwen2.5-Coder-0.5B-Instruct', revision=None, inference_mode=True, r=8, target_modules={'o_proj', 'k_proj', 'q_proj', 'up_proj', 'v_proj', 'gate_proj', 'down_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None)}\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# Sanity check: model type should be a PEFT wrapper (not raw Qwen model)\n",
    "print(\"Model class:\", type(model))\n",
    "\n",
    "# LoRA config should be visible in the model\n",
    "print(\"PEFT config:\", getattr(model, \"peft_config\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fine-tuned model ready for dev inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfb183fc7664527a21f6c2891e71742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üîÅ Generating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "# === DEV INFERENCE: use fine-tuned Qwen (base + LoRA) ===\n",
    "import os, re, json, torch\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "# ---- Config ----\n",
    "BASE_DIR      = \"/workspace\"\n",
    "CACHE_DIR     = os.getenv(\"HF_HOME\", f\"{BASE_DIR}/hf_cache\")\n",
    "QWEN_MODEL_ID = \"Qwen/Qwen2.5-Coder-14B-Instruct\"   # must match training\n",
    "OUTPUT_DIR    = f\"{BASE_DIR}/qwen2.5-14b-qlora\"    # LoRA adapter dir (from TrainingArguments)\n",
    "DEV_IN_PATH   = f\"gpt_translated_test_data.csv\"  # must contain: id,instruction\n",
    "PRED_JSON     = f\"{BASE_DIR}/dev_predictions.json\"\n",
    "SUB_JSON      = f\"{BASE_DIR}/submission.json\"\n",
    "\n",
    "# ---- Load tokenizer & base model ----\n",
    "compute_dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "_base = AutoModelForCausalLM.from_pretrained(\n",
    "    QWEN_MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=compute_dtype,\n",
    "    trust_remote_code=True,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "# ---- Attach trained LoRA adapters ----\n",
    "model = PeftModel.from_pretrained(_base, OUTPUT_DIR)\n",
    "model.eval()\n",
    "print(\"‚úÖ Fine-tuned model ready for dev inference.\")\n",
    "\n",
    "# ---- Prompt rendering ----\n",
    "def render_prompt(instruction: str) -> str:\n",
    "    msgs = [\n",
    "        {\"role\": \"system\", \"content\": guideline},  # replace with your guideline if defined\n",
    "        {\"role\": \"user\",  \"content\": instruction.strip()}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "CODE_BLOCK_RE = re.compile(r\"```python\\s*([\\s\\S]*?)```\", re.IGNORECASE)\n",
    "def extract_code(text: str) -> str:\n",
    "    m = CODE_BLOCK_RE.search(text)\n",
    "    return m.group(1).strip() if m else text.strip()\n",
    "\n",
    "# ---- Inference function ----\n",
    "@torch.inference_mode()\n",
    "def generate_code(instruction: str,\n",
    "                  max_new_tokens=512,\n",
    "                  rep_penalty=1.05):\n",
    "    text = render_prompt(instruction)\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    out_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,        # deterministic (greedy decoding)\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        repetition_penalty=rep_penalty,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    gen = tokenizer.decode(out_ids[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return extract_code(gen)\n",
    "\n",
    "# ---- Run inference over dev set ----\n",
    "df = pd.read_csv(DEV_IN_PATH)\n",
    "\n",
    "###############\n",
    "# df = df.sample(20)\n",
    "\n",
    "assert {\"id\", \"instruction\"}.issubset(df.columns), \"dev_translated.csv must have id,instruction\"\n",
    "\n",
    "pred_rows = []\n",
    "for rid, instr in tqdm(zip(df[\"id\"], df[\"instruction\"]), total=len(df), desc=\"üîÅ Generating\"):\n",
    "    code = generate_code(instr)\n",
    "    fenced = f\"```python\\n{code}\\n```\"\n",
    "    pred_rows.append({\"id\": int(rid), \"instruction\": instr, \"response\": fenced})\n",
    "\n",
    "# ---- Save predictions ----\n",
    "with open(PRED_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pred_rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "sub_rows = [{\"id\": r[\"id\"], \"response\": r[\"response\"]} for r in pred_rows]\n",
    "with open(SUB_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sub_rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Wrote {PRED_JSON}, {SUB_JSON}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, re, zipfile\n",
    "\n",
    "SUB_PATH = \"/workspace/submission.json\"\n",
    "\n",
    "def file_format_check(path: str) -> bool:\n",
    "    # name + extension\n",
    "    if os.path.basename(path) != \"submission.json\":\n",
    "        print(\"Error: File name must be exactly 'submission.json'\")\n",
    "        return False\n",
    "    if not path.lower().endswith(\".json\"):\n",
    "        print(\"Error: File must have .json extension\")\n",
    "        return False\n",
    "\n",
    "    # must be valid JSON (not JSONL) and root must be a list\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Invalid JSON format - {e}\")\n",
    "        print(\"Note: The file must be in proper JSON format (not JSONL)\")\n",
    "        return False\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        print(\"Error: The root element should be a list of objects\")\n",
    "        return False\n",
    "\n",
    "    # each item: dict with ONLY keys {'id','response'}; id=int; response=str\n",
    "    for idx, item in enumerate(data):\n",
    "        if not isinstance(item, dict):\n",
    "            print(f\"Error: Item at index {idx} is not a dictionary\")\n",
    "            return False\n",
    "        keys = set(item.keys())\n",
    "        if keys != {\"id\", \"response\"}:\n",
    "            print(f\"Error: Item at index {idx} must contain only keys 'id' and 'response', found: {keys}\")\n",
    "            return False\n",
    "        if not isinstance(item[\"id\"], int):\n",
    "            print(f\"Error: 'id' field at index {idx} must be an integer\")\n",
    "            return False\n",
    "        if not isinstance(item[\"response\"], str):\n",
    "            print(f\"Error: 'response' field at index {idx} must be a string\")\n",
    "            return False\n",
    "\n",
    "    print(\"Format check passed successfully!\")\n",
    "    return True\n",
    "\n",
    "# ---------- Load, compute per-item validity, blank invalids, save, zip ----------\n",
    "# Load JSON list\n",
    "with open(SUB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "n = len(data)\n",
    "fence_pat = re.compile(r\"^```python[\\s\\S]*```$\", re.MULTILINE)\n",
    "\n",
    "valid_format = []\n",
    "valid_fence  = []\n",
    "valid_both   = []\n",
    "\n",
    "# Per-item validation mirrors file checker semantics\n",
    "def item_format_ok(item):\n",
    "    return (\n",
    "        isinstance(item, dict)\n",
    "        and set(item.keys()) == {\"id\", \"response\"}\n",
    "        and isinstance(item[\"id\"], int)\n",
    "        and isinstance(item[\"response\"], str)\n",
    "    )\n",
    "\n",
    "for item in data:\n",
    "    vfmt = item_format_ok(item)\n",
    "    vf   = bool(fence_pat.match(item[\"response\"])) if vfmt else False\n",
    "    valid_format.append(vfmt)\n",
    "    valid_fence.append(vf)\n",
    "    valid_both.append(vfmt and vf)\n",
    "\n",
    "# After computing valid_format, valid_fence, valid_both\n",
    "for i, item in enumerate(data):\n",
    "    if not valid_format[i]:\n",
    "        print(f\"‚ùå Format Error at index {i}: {item}\")\n",
    "    elif not valid_fence[i]:\n",
    "        print(f\"‚ùå Fencing Error at index {i} (id={item.get('id')}):\")\n",
    "        print(item[\"response\"])\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "# Report stats\n",
    "nf = sum(valid_fence)\n",
    "nm = sum(valid_format)\n",
    "nb = sum(valid_both)\n",
    "den = max(n, 1)\n",
    "print(f\"Fencing valid: {nf}/{n} ({nf*100.0/den:.1f}%)\")\n",
    "print(f\"Format valid:  {nm}/{n} ({nm*100.0/den:.1f}%)\")\n",
    "print(f\"Both valid:    {nb}/{n} ({nb*100.0/den:.1f}%)\")\n",
    "\n",
    "# Strict policy: blank responses that fail ANY check\n",
    "for i, ok in enumerate(valid_both):\n",
    "    if not ok and isinstance(data[i], dict) and \"response\" in data[i]:\n",
    "        data[i][\"response\"] = \"\"\n",
    "\n",
    "# Overwrite submission.json (id+response only)\n",
    "with open(SUB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        [{\"id\": item[\"id\"], \"response\": item[\"response\"]} for item in data],\n",
    "        f, ensure_ascii=False, indent=2\n",
    "    )\n",
    "print(\"‚úÖ Updated submission.json after checks (invalid responses blanked).\")\n",
    "\n",
    "# Final file-level check (should pass)\n",
    "_ = file_format_check(SUB_PATH)\n",
    "\n",
    "# Zip as submission.zip (Jupyter-friendly, no shell commands)\n",
    "with zipfile.ZipFile(\"submission.zip\", \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(SUB_PATH)\n",
    "print(\"üì¶ Created submission.zip containing submission.json.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
